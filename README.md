# deep-learning-challenge
Class notes and discussions were leveraged to complete this assignment.

The following sources were used to get a better understanding of dropout layers and leaky ReLU.

Dropout Layers:
- [Medium](https://towardsdatascience.com/dropout-in-neural-networks-47a162d621d9)
- [Keras](https://keras.io/api/layers/regularization_layers/dropout/)
- [MathWorks](https://www.mathworks.com/help/deeplearning/ref/nnet.cnn.layer.dropoutlayer.html)

Leaky ReLU:
- [Medium](https://medium.com/@sreeku.ralla/activation-functions-relu-vs-leaky-relu-b8272dc0b1be)
- [StackExchange](https://datascience.stackexchange.com/questions/39042/how-to-use-leakyrelu-as-activation-function-in-sequence-dnn-in-keraswhen-it-per)


